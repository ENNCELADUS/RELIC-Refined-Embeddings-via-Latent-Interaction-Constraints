W0212 00:45:07.512000 148778 site-packages/torch/distributed/run.py:774] 
W0212 00:45:07.512000 148778 site-packages/torch/distributed/run.py:774] *****************************************
W0212 00:45:07.512000 148778 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0212 00:45:07.512000 148778 site-packages/torch/distributed/run.py:774] *****************************************
INFO:__main__:Loaded config: configs/v3.yaml
INFO:src.utils.distributed:Initialized distributed process group (backend=nccl rank=0 local_rank=0 world_size=4).
2026-02-12 00:45:13 INFO relic.v3.train.20260212_004513.rank0 - pipeline_bootstrap | ddp_enabled=True | is_distributed=True | local_rank=0 | log_dir=logs/v3/train/20260212_004513 | mode=full_pipeline | rank=0 | run_id=20260212_004513 | seed=47 | stage=train | world_size=4
[rank1]:[W212 00:45:13.871773032 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
2026-02-12 00:45:13 INFO relic.v3.evaluate.20260212_004513.rank0 - pipeline_bootstrap | ddp_enabled=True | is_distributed=True | local_rank=0 | log_dir=logs/v3/evaluate/20260212_004513 | mode=full_pipeline | rank=0 | run_id=20260212_004513 | seed=47 | stage=evaluate | world_size=4
2026-02-12 00:45:13 INFO relic.v3.train.20260212_004513.rank0 - device_resolved | requested_device=cuda | resolved_device=cuda:0
2026-02-12 00:45:13 INFO relic.v3.evaluate.20260212_004513.rank0 - device_resolved | requested_device=cuda | resolved_device=cuda:0
[rank3]:[W212 00:45:13.878977664 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 3]  using GPU 3 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
[rank2]:[W212 00:45:13.899962380 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 2]  using GPU 2 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
INFO:src.embed.embed:Generating embeddings for 583 proteins using model=esm3_sm_open_v1 device=cuda
INFO:src.embed.embed:Embedded 100/583 proteins
INFO:src.embed.embed:Embedded 200/583 proteins
INFO:src.embed.embed:Embedded 300/583 proteins
INFO:src.embed.embed:Embedded 400/583 proteins
INFO:src.embed.embed:Embedded 500/583 proteins
INFO:src.embed.embed:Embedded 583/583 proteins
WARNING:py.warnings:/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once

[rank0]:[W212 00:46:15.936576368 ProcessGroupNCCL.cpp:5023] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 as device used by this process is currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. You can specify device_id in init_process_group() to force use of a particular device.
2026-02-12 00:46:17 INFO relic.v3.train.20260212_004513.rank0 - dataloaders_ready | test_batches=8 | test_samples=128 | train_batches=2 | train_samples=128 | valid_batches=8 | valid_samples=128
2026-02-12 00:46:17 INFO relic.v3.evaluate.20260212_004513.rank0 - dataloaders_ready | test_batches=8 | test_samples=128 | train_batches=2 | train_samples=128 | valid_batches=8 | valid_samples=128
2026-02-12 00:46:17 INFO relic.v3.train.20260212_004513.rank0 - model_initialized | model_name=v3 | parameter_count=10197633
2026-02-12 00:46:20 INFO relic.v3.train.20260212_004513.rank0 - ddp_wrap_complete | find_unused_parameters=True | model_wrapped=True
2026-02-12 00:46:20 INFO relic.v3.evaluate.20260212_004513.rank0 - ddp_wrap_complete | find_unused_parameters=True | model_wrapped=True
2026-02-12 00:46:20 INFO relic.v3.train.20260212_004513.rank0 - stage_boundary_start | stage=train
2026-02-12 00:46:20 INFO relic.v3.train.20260212_004513.rank0 - stage_start | log_dir=logs/v3/train/20260212_004513 | model_dir=models/v3/train/20260212_004513 | run_id=20260212_004513 | stage=train
2026-02-12 00:46:20 INFO relic.v3.train.20260212_004513.rank0 - training_config_applied | csv_path=logs/v3/train/20260212_004513/training_step.csv | epochs=3 | heartbeat_every_n_steps=20 | monitor_metric=auprc | save_best_only=True | validation_metrics=auprc,auroc
2026-02-12 00:46:20 INFO relic.v3.train.20260212_004513.rank0 - epoch_start | epoch=1 | total_epochs=3
2026-02-12 00:46:21 INFO relic.v3.train.20260212_004513.rank0 - Epoch 1 step 1/2 | running_train_loss=0.7298 | lr=0.000047
2026-02-12 00:46:21 INFO relic.v3.train.20260212_004513.rank0 - Epoch 1 step 2/2 | running_train_loss=0.7230 | lr=0.000035
2026-02-12 00:46:21 INFO relic.v3.train.20260212_004513.rank0 - training_csv_row_written | epoch=1 | path=logs/v3/train/20260212_004513/training_step.csv
2026-02-12 00:46:21 INFO relic.v3.train.20260212_004513.rank0 - checkpoint_saved_best | epoch=1 | monitor_metric=val_auprc | monitor_value=0.482360 | path=models/v3/train/20260212_004513/best_model.pth
2026-02-12 00:46:21 INFO relic.v3.train.20260212_004513.rank0 - epoch_complete | epoch=1 | epoch_seconds=1.138384 | monitor_metric=val_auprc | monitor_value=0.482360 | train_loss=0.722966 | val_loss=0.752409
2026-02-12 00:46:21 INFO relic.v3.train.20260212_004513.rank0 - epoch_start | epoch=2 | total_epochs=3
[rank2]:[W212 00:46:21.209688591 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank3]:[W212 00:46:21.214161018 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank1]:[W212 00:46:21.218257621 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[rank0]:[W212 00:46:21.219769607 reducer.cpp:1457] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
WARNING:py.warnings:/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:192: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn(

2026-02-12 00:46:21 INFO relic.v3.train.20260212_004513.rank0 - Epoch 2 step 1/2 | running_train_loss=1.0345 | lr=0.000047
2026-02-12 00:46:22 INFO relic.v3.train.20260212_004513.rank0 - Epoch 2 step 2/2 | running_train_loss=1.0055 | lr=0.000035
2026-02-12 00:46:22 INFO relic.v3.train.20260212_004513.rank0 - training_csv_row_written | epoch=2 | path=logs/v3/train/20260212_004513/training_step.csv
2026-02-12 00:46:22 INFO relic.v3.train.20260212_004513.rank0 - epoch_complete | epoch=2 | epoch_seconds=0.697059 | monitor_metric=val_auprc | monitor_value=0.482360 | train_loss=1.005513 | val_loss=0.752409
2026-02-12 00:46:22 INFO relic.v3.train.20260212_004513.rank0 - epoch_start | epoch=3 | total_epochs=3
2026-02-12 00:46:22 INFO relic.v3.train.20260212_004513.rank0 - Epoch 3 step 1/2 | running_train_loss=0.8172 | lr=0.000019
2026-02-12 00:46:22 INFO relic.v3.train.20260212_004513.rank0 - Epoch 3 step 2/2 | running_train_loss=0.8841 | lr=0.000005
2026-02-12 00:46:23 INFO relic.v3.train.20260212_004513.rank0 - training_csv_row_written | epoch=3 | path=logs/v3/train/20260212_004513/training_step.csv
2026-02-12 00:46:23 INFO relic.v3.train.20260212_004513.rank0 - epoch_complete | epoch=3 | epoch_seconds=0.660336 | monitor_metric=val_auprc | monitor_value=0.481438 | train_loss=0.884092 | val_loss=0.693280
2026-02-12 00:46:23 INFO relic.v3.train.20260212_004513.rank0 - early_stopping_triggered | epoch=3
2026-02-12 00:46:23 INFO relic.v3.train.20260212_004513.rank0 - stage_complete | best_checkpoint_path=models/v3/train/20260212_004513/best_model.pth | run_id=20260212_004513 | stage=train | training_csv_path=logs/v3/train/20260212_004513/training_step.csv
2026-02-12 00:46:23 INFO relic.v3.train.20260212_004513.rank0 - stage_boundary_end | checkpoint_path=models/v3/train/20260212_004513/best_model.pth | stage=train
2026-02-12 00:46:23 INFO relic.v3.evaluate.20260212_004513.rank0 - stage_boundary_start | stage=evaluate
2026-02-12 00:46:23 INFO relic.v3.evaluate.20260212_004513.rank0 - stage_start | checkpoint_path=models/v3/train/20260212_004513/best_model.pth | log_dir=logs/v3/evaluate/20260212_004513 | run_id=20260212_004513 | stage=evaluate
2026-02-12 00:46:23 INFO relic.v3.evaluate.20260212_004513.rank0 - checkpoint_load_complete | path=models/v3/train/20260212_004513/best_model.pth
WARNING:py.warnings:/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.
  warnings.warn(

2026-02-12 00:46:23 INFO relic.v3.evaluate.20260212_004513.rank0 - evaluation_metrics | accuracy=1.000000 | auprc=0.000000 | auroc=0.000000 | f1=1.000000 | mcc=0.000000 | precision=1.000000 | recall=1.000000 | sensitivity=1.000000 | specificity=0.000000 | split=test
2026-02-12 00:46:23 INFO relic.v3.evaluate.20260212_004513.rank0 - evaluation_csv_written | path=logs/v3/evaluate/20260212_004513/evaluate.csv
2026-02-12 00:46:23 INFO relic.v3.evaluate.20260212_004513.rank0 - stage_complete | evaluate_csv_path=logs/v3/evaluate/20260212_004513/evaluate.csv | run_id=20260212_004513 | stage=evaluate
2026-02-12 00:46:23 INFO relic.v3.evaluate.20260212_004513.rank0 - stage_boundary_end | stage=evaluate
