run_config:
  mode: "full_pipeline"  # full_pipeline | train_only | eval_only
  seed: 47
  train_run_id: null
  finetune_run_id: null
  eval_run_id: null
  load_checkpoint_path: null
  save_best_only: true

device_config:
  device: "cuda"
  ddp_enabled: true
  use_mixed_precision: true

data_config:
  benchmark:
    name: "TMP"
    root_dir: "data/TMP"
    processed_dir: "data/TMP/processed"
  embeddings:
    source: "esm3"
    cache_dir: "data/embeddings/esm3"
  max_sequence_length: 1024
  dataloader:
    train_dataset: "data/TMP/processed/pretrain_train_balanced.csv"
    valid_dataset: "data/TMP/processed/pretrain_val_balanced.csv"
    finetune_train_dataset: "data/TMP/processed/finetune_train.csv"
    finetune_val_dataset: "data/TMP/processed/finetune_val.csv"
    test_dataset: "data/TMP/processed/test.csv"
    num_workers: 4
    pin_memory: true
    drop_last: true
    sampling:
      strategy: "ohem"  # none | ohem
      warmup_epochs: 2
      pool_multiplier: 16
      cap_protein: 2

model_config:
  model: "v5"
  input_dim: 1536
  d_model: 192
  encoder_layers: 2
  cross_attn_layers: 2
  n_heads: 8
  pair_dim: 96
  cnn_dim: 64
  cnn_blocks: 2
  interaction_map:
    include_pair_features: true
    similarity: "cosine"
    eps: 1.0e-8
  pooling:
    mode: "max_mean"
  mlp_head:
    hidden_dims: [128, 64]
    dropout: 0.20
    activation: "gelu"
    norm: "layernorm"
  regularization:
    dropout: 0.15
    token_dropout: 0.10
    cross_attention_dropout: 0.15
    stochastic_depth: 0.10
    cnn_dropout: 0.10

training_config:
  epochs: 20
  batch_size: 16
  early_stopping_patience: 5
  monitor_metric: "auprc"
  logging:
    validation_metrics: ["auprc", "auroc", "f1", "accuracy"]
  optimizer:
    type: "adamw"
    lr: 0.00006
    beta1: 0.9
    beta2: 0.999
    eps: 1.0e-8
    weight_decay: 0.015
  scheduler:
    type: "onecycle"
    max_lr: 0.00006
    pct_start: 0.20
    div_factor: 25
    final_div_factor: 10000
    anneal_strategy: "cos"
  loss:
    type: "bce_with_logits"
    pos_weight: 1.0
    label_smoothing: 0.05
  strategy:
    type: "none"

evaluate:
  metrics:
    - "auroc"
    - "auprc"
    - "accuracy"
    - "sensitivity"
    - "specificity"
    - "precision"
    - "recall"
    - "f1"
    - "mcc"

optimization:
  enabled: true
  backend: "optuna"
  study_name: "v5_hpo"
  objective_metric: "val_auprc"
  direction: "maximize"
  budget:
    n_trials: 30
    timeout_minutes: 720
  execution:
    trial_mode: "train_only"
    ddp_per_trial: false
    catch_oom_as_pruned: true
  storage:
    type: "sqlite"
    url: "sqlite:///artifacts/hpo/v5_hpo.db"
  sampler:
    name: "TPESampler"
    seed: 47
  pruner:
    name: "MedianPruner"
    n_startup_trials: 5
    n_warmup_steps: 3
  search_space:
    - name: "optimizer_lr"
      path: "training_config.optimizer.lr"
      type: "float"
      low: 1.0e-5
      high: 3.0e-4
      log: true
    - name: "weight_decay"
      path: "training_config.optimizer.weight_decay"
      type: "float"
      low: 1.0e-4
      high: 1.0e-1
      log: true
    - name: "batch_size"
      path: "training_config.batch_size"
      type: "categorical"
      choices: [8, 16, 32]
    - name: "d_model"
      path: "model_config.d_model"
      type: "categorical"
      choices: [128, 192, 256]
    - name: "encoder_layers"
      path: "model_config.encoder_layers"
      type: "int"
      low: 1
      high: 4
    - name: "cross_attn_layers"
      path: "model_config.cross_attn_layers"
      type: "int"
      low: 1
      high: 4

nas_lite:
  enabled: true
  method: "arch_params_hpo"
  max_candidates: 20
