W0211 23:01:40.841000 121579 site-packages/torch/distributed/run.py:774] 
W0211 23:01:40.841000 121579 site-packages/torch/distributed/run.py:774] *****************************************
W0211 23:01:40.841000 121579 site-packages/torch/distributed/run.py:774] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0211 23:01:40.841000 121579 site-packages/torch/distributed/run.py:774] *****************************************
INFO:__main__:Loaded config: tests/e2e/artifacts/v3_hpc.yaml
INFO:__main__:Loaded config: tests/e2e/artifacts/v3_hpc.yaml
INFO:__main__:Loaded config: tests/e2e/artifacts/v3_hpc.yaml
INFO:__main__:Loaded config: tests/e2e/artifacts/v3_hpc.yaml
INFO:src.utils.distributed:Initialized distributed process group (backend=nccl rank=0 local_rank=0 world_size=4).
2026-02-11 23:01:47 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - pipeline_bootstrap | ddp_enabled=True | is_distributed=True | local_rank=0 | log_dir=logs/v3/pretrain/hpc_e2e_pretrain | mode=full_pipeline | rank=0 | run_id=hpc_e2e_pretrain | seed=47 | stage=pretrain | world_size=4
INFO:src.utils.distributed:Initialized distributed process group (backend=nccl rank=2 local_rank=2 world_size=4).
2026-02-11 23:01:47 INFO relic.v3.finetune.hpc_e2e_finetune.rank0 - pipeline_bootstrap | ddp_enabled=True | is_distributed=True | local_rank=0 | log_dir=logs/v3/finetune/hpc_e2e_finetune | mode=full_pipeline | rank=0 | run_id=hpc_e2e_finetune | seed=47 | stage=finetune | world_size=4
INFO:src.utils.distributed:Initialized distributed process group (backend=nccl rank=1 local_rank=1 world_size=4).
INFO:src.utils.distributed:Initialized distributed process group (backend=nccl rank=3 local_rank=3 world_size=4).
2026-02-11 23:01:47 INFO relic.v3.evaluate.hpc_e2e_eval.rank0 - pipeline_bootstrap | ddp_enabled=True | is_distributed=True | local_rank=0 | log_dir=logs/v3/evaluate/hpc_e2e_eval | mode=full_pipeline | rank=0 | run_id=hpc_e2e_eval | seed=47 | stage=evaluate | world_size=4
2026-02-11 23:01:47 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - device_resolved | requested_device=cuda | resolved_device=cuda:0
2026-02-11 23:01:47 INFO relic.v3.finetune.hpc_e2e_finetune.rank0 - device_resolved | requested_device=cuda | resolved_device=cuda:0
2026-02-11 23:01:47 INFO relic.v3.evaluate.hpc_e2e_eval.rank0 - device_resolved | requested_device=cuda | resolved_device=cuda:0
2026-02-11 23:01:47 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - dataloaders_ready | test_batches=1 | test_samples=32 | train_batches=1 | train_samples=32 | valid_batches=1 | valid_samples=32
2026-02-11 23:01:47 INFO relic.v3.finetune.hpc_e2e_finetune.rank0 - dataloaders_ready | test_batches=1 | test_samples=32 | train_batches=1 | train_samples=32 | valid_batches=1 | valid_samples=32
2026-02-11 23:01:47 INFO relic.v3.evaluate.hpc_e2e_eval.rank0 - dataloaders_ready | test_batches=1 | test_samples=32 | train_batches=1 | train_samples=32 | valid_batches=1 | valid_samples=32
2026-02-11 23:01:47 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - model_initialized | model_name=v3 | parameter_count=234721
2026-02-11 23:01:50 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - ddp_wrap_complete | model_wrapped=True
2026-02-11 23:01:50 INFO relic.v3.finetune.hpc_e2e_finetune.rank0 - ddp_wrap_complete | model_wrapped=True
2026-02-11 23:01:50 INFO relic.v3.evaluate.hpc_e2e_eval.rank0 - ddp_wrap_complete | model_wrapped=True
2026-02-11 23:01:50 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - stage_boundary_start | stage=pretrain
2026-02-11 23:01:50 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - stage_start | log_dir=logs/v3/pretrain/hpc_e2e_pretrain | model_dir=models/v3/pretrain/hpc_e2e_pretrain | run_id=hpc_e2e_pretrain | stage=pretrain
2026-02-11 23:01:50 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - training_config_applied | csv_path=logs/v3/pretrain/hpc_e2e_pretrain/training_step.csv | epochs=1 | heartbeat_every_n_steps=20 | monitor_metric=auprc | save_best_only=True | validation_metrics=auprc,auroc
2026-02-11 23:01:50 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - epoch_start | epoch=1 | total_epochs=1
2026-02-11 23:01:51 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - Epoch 1 step 1/1 | running_train_loss=0.6096 | lr=0.000043
2026-02-11 23:01:53 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - training_csv_row_written | epoch=1 | path=logs/v3/pretrain/hpc_e2e_pretrain/training_step.csv
2026-02-11 23:01:53 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - checkpoint_saved_best | epoch=1 | monitor_metric=val_auprc | monitor_value=0.631196 | path=models/v3/pretrain/hpc_e2e_pretrain/best_model.pth
2026-02-11 23:01:53 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - epoch_complete | epoch=1 | epoch_seconds=2.376341 | monitor_metric=val_auprc | monitor_value=0.631196 | train_loss=0.609604 | val_loss=0.682790
2026-02-11 23:01:53 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - stage_complete | best_checkpoint_path=models/v3/pretrain/hpc_e2e_pretrain/best_model.pth | run_id=hpc_e2e_pretrain | stage=pretrain | training_csv_path=logs/v3/pretrain/hpc_e2e_pretrain/training_step.csv
/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
2026-02-11 23:01:53 INFO relic.v3.pretrain.hpc_e2e_pretrain.rank0 - stage_boundary_end | checkpoint_path=models/v3/pretrain/hpc_e2e_pretrain/best_model.pth | stage=pretrain
2026-02-11 23:01:53 INFO relic.v3.finetune.hpc_e2e_finetune.rank0 - stage_boundary_start | stage=finetune
2026-02-11 23:01:53 INFO relic.v3.finetune.hpc_e2e_finetune.rank0 - stage_start | log_dir=logs/v3/finetune/hpc_e2e_finetune | model_dir=models/v3/finetune/hpc_e2e_finetune | run_id=hpc_e2e_finetune | stage=finetune
2026-02-11 23:01:53 INFO relic.v3.finetune.hpc_e2e_finetune.rank0 - checkpoint_load_start | path=models/v3/pretrain/hpc_e2e_pretrain/best_model.pth
2026-02-11 23:01:53 INFO relic.v3.finetune.hpc_e2e_finetune.rank0 - checkpoint_load_complete | path=models/v3/pretrain/hpc_e2e_pretrain/best_model.pth
2026-02-11 23:01:53 INFO relic.v3.finetune.hpc_e2e_finetune.rank0 - training_config_applied | csv_path=logs/v3/finetune/hpc_e2e_finetune/training_step.csv | epochs=1 | heartbeat_every_n_steps=20 | monitor_metric=auprc | save_best_only=True | validation_metrics=auprc,auroc
2026-02-11 23:01:53 INFO relic.v3.finetune.hpc_e2e_finetune.rank0 - epoch_start | epoch=1 | total_epochs=1
[rank3]: Traceback (most recent call last):
[rank3]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank3]:     return _run_code(code, main_globals, None,
[rank3]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/runpy.py", line 86, in _run_code
[rank3]:     exec(code, run_globals)
[rank3]:   File "/public/home/wangar2023/relic/src/run.py", line 887, in <module>
[rank3]:     main()
[rank3]:   File "/public/home/wangar2023/relic/src/run.py", line 883, in main
[rank3]:     execute_pipeline(config=config)
[rank3]:   File "/public/home/wangar2023/relic/src/run.py", line 823, in execute_pipeline
[rank3]:     finetune_checkpoint = run_training_stage(
[rank3]:   File "/public/home/wangar2023/relic/src/run.py", line 499, in run_training_stage
[rank3]:     train_stats = trainer.train_one_epoch(dataloaders["train"], epoch_index=epoch)
[rank3]:   File "/public/home/wangar2023/relic/src/train/base.py", line 208, in train_one_epoch
[rank3]:     output = self._forward_model(prepared_batch)
[rank3]:   File "/public/home/wangar2023/relic/src/train/base.py", line 150, in _forward_model
[rank3]:     output = self.model(**batch)
[rank3]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank3]:     return self._call_impl(*args, **kwargs)
[rank3]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank3]:     return forward_call(*args, **kwargs)
[rank3]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1644, in forward
[rank3]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank3]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1533, in _pre_forward
[rank3]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank3]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank3]: making sure all `forward` function outputs participate in calculating loss. 
[rank3]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank3]: Parameter indices which did not receive grad for rank 3: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40
[rank3]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[rank0]: Traceback (most recent call last):
[rank0]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank0]:     return _run_code(code, main_globals, None,
[rank0]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/runpy.py", line 86, in _run_code
[rank0]:     exec(code, run_globals)
[rank0]:   File "/public/home/wangar2023/relic/src/run.py", line 887, in <module>
[rank0]:     main()
[rank0]:   File "/public/home/wangar2023/relic/src/run.py", line 883, in main
[rank0]:     execute_pipeline(config=config)
[rank0]:   File "/public/home/wangar2023/relic/src/run.py", line 823, in execute_pipeline
[rank0]:     finetune_checkpoint = run_training_stage(
[rank0]:   File "/public/home/wangar2023/relic/src/run.py", line 499, in run_training_stage
[rank0]:     train_stats = trainer.train_one_epoch(dataloaders["train"], epoch_index=epoch)
[rank0]:   File "/public/home/wangar2023/relic/src/train/base.py", line 208, in train_one_epoch
[rank0]:     output = self._forward_model(prepared_batch)
[rank0]:   File "/public/home/wangar2023/relic/src/train/base.py", line 150, in _forward_model
[rank0]:     output = self.model(**batch)
[rank0]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1644, in forward
[rank0]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank0]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1533, in _pre_forward
[rank0]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank0]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank0]: making sure all `forward` function outputs participate in calculating loss. 
[rank0]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank0]: Parameter indices which did not receive grad for rank 0: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40
[rank0]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[rank1]: Traceback (most recent call last):
[rank1]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank1]:     return _run_code(code, main_globals, None,
[rank1]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/runpy.py", line 86, in _run_code
[rank1]:     exec(code, run_globals)
[rank1]:   File "/public/home/wangar2023/relic/src/run.py", line 887, in <module>
[rank1]:     main()
[rank1]:   File "/public/home/wangar2023/relic/src/run.py", line 883, in main
[rank1]:     execute_pipeline(config=config)
[rank1]:   File "/public/home/wangar2023/relic/src/run.py", line 823, in execute_pipeline
[rank1]:     finetune_checkpoint = run_training_stage(
[rank1]:   File "/public/home/wangar2023/relic/src/run.py", line 499, in run_training_stage
[rank1]:     train_stats = trainer.train_one_epoch(dataloaders["train"], epoch_index=epoch)
[rank1]:   File "/public/home/wangar2023/relic/src/train/base.py", line 208, in train_one_epoch
[rank1]:     output = self._forward_model(prepared_batch)
[rank1]:   File "/public/home/wangar2023/relic/src/train/base.py", line 150, in _forward_model
[rank1]:     output = self.model(**batch)
[rank1]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1644, in forward
[rank1]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank1]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1533, in _pre_forward
[rank1]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank1]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank1]: making sure all `forward` function outputs participate in calculating loss. 
[rank1]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank1]: Parameter indices which did not receive grad for rank 1: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40
[rank1]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
[rank2]: Traceback (most recent call last):
[rank2]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/runpy.py", line 196, in _run_module_as_main
[rank2]:     return _run_code(code, main_globals, None,
[rank2]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/runpy.py", line 86, in _run_code
[rank2]:     exec(code, run_globals)
[rank2]:   File "/public/home/wangar2023/relic/src/run.py", line 887, in <module>
[rank2]:     main()
[rank2]:   File "/public/home/wangar2023/relic/src/run.py", line 883, in main
[rank2]:     execute_pipeline(config=config)
[rank2]:   File "/public/home/wangar2023/relic/src/run.py", line 823, in execute_pipeline
[rank2]:     finetune_checkpoint = run_training_stage(
[rank2]:   File "/public/home/wangar2023/relic/src/run.py", line 499, in run_training_stage
[rank2]:     train_stats = trainer.train_one_epoch(dataloaders["train"], epoch_index=epoch)
[rank2]:   File "/public/home/wangar2023/relic/src/train/base.py", line 208, in train_one_epoch
[rank2]:     output = self._forward_model(prepared_batch)
[rank2]:   File "/public/home/wangar2023/relic/src/train/base.py", line 150, in _forward_model
[rank2]:     output = self.model(**batch)
[rank2]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1773, in _wrapped_call_impl
[rank2]:     return self._call_impl(*args, **kwargs)
[rank2]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1784, in _call_impl
[rank2]:     return forward_call(*args, **kwargs)
[rank2]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1644, in forward
[rank2]:     inputs, kwargs = self._pre_forward(*inputs, **kwargs)
[rank2]:   File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1533, in _pre_forward
[rank2]:     if torch.is_grad_enabled() and self.reducer._rebuild_buckets():
[rank2]: RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one. This error indicates that your module has parameters that were not used in producing loss. You can enable unused parameter detection by passing the keyword argument `find_unused_parameters=True` to `torch.nn.parallel.DistributedDataParallel`, and by 
[rank2]: making sure all `forward` function outputs participate in calculating loss. 
[rank2]: If you already have done the above, then the distributed data parallel module wasn't able to locate the output tensors in the return value of your module's `forward` function. Please include the loss function and the structure of the return value of `forward` of your module when reporting this issue (e.g. list, dict, iterable).
[rank2]: Parameter indices which did not receive grad for rank 2: 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40
[rank2]:  In addition, you can set the environment variable TORCH_DISTRIBUTED_DEBUG to either INFO or DETAIL to print out information about which particular parameters did not receive gradient on this rank as part of this error
W0211 23:01:54.963000 121579 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 121838 closing signal SIGTERM
W0211 23:01:54.964000 121579 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 121839 closing signal SIGTERM
W0211 23:01:54.964000 121579 site-packages/torch/distributed/elastic/multiprocessing/api.py:900] Sending process 121840 closing signal SIGTERM
E0211 23:01:55.192000 121579 site-packages/torch/distributed/elastic/multiprocessing/api.py:874] failed (exitcode: 1) local_rank: 0 (pid: 121837) of binary: /public/home/wangar2023/.conda/envs/esm/bin/python3.1
Traceback (most recent call last):
  File "/public/home/wangar2023/.conda/envs/esm/bin/torchrun", line 7, in <module>
    sys.exit(main())
  File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 357, in wrapper
    return f(*args, **kwargs)
  File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 143, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/public/home/wangar2023/.conda/envs/esm/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 277, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
src.run FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2026-02-11_23:01:54
  host      : ai_gpu30
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 121837)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
