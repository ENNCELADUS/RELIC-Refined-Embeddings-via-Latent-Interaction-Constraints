run_config:
  mode: "full_pipeline"  # full_pipeline | train_only | eval_only
  seed: 47
  train_run_id: null
  finetune_run_id: null
  eval_run_id: null
  load_checkpoint_path: null
  save_best_only: true

device_config:
  device: "cuda"
  ddp_enabled: true
  use_mixed_precision: true

data_config:
  benchmark:
    name: "TMP"
    root_dir: "data/TMP"
    processed_dir: "data/TMP/processed"
  embeddings:
    source: "esm3"
    cache_dir: "data/embeddings/esm3"
    sequence_file: "data/TMP/processed/proteins.csv"
    id_column: "protein_id"
    sequence_column: "sequence"
  max_sequence_length: 1024
  dataloader:
    train_dataset: "data/TMP/processed/pretrain_train_balanced.csv"
    valid_dataset: "data/TMP/processed/pretrain_val_balanced.csv"
    finetune_train_dataset: "data/TMP/processed/finetune_train.csv"
    finetune_val_dataset: "data/TMP/processed/finetune_val.csv"
    test_dataset: "data/TMP/processed/test.csv"
    num_workers: 4
    pin_memory: true
    drop_last: true
    sampling:
      strategy: "ohem"  # none | ohem
      warmup_epochs: 2
      pool_multiplier: 16
      cap_protein: 2

model_config:
  model: "v6"
  input_dim: 1536
  d_model: 256
  cross_attn_layers: 2
  n_heads: 8
  esm3:
    model_name: "esm3_sm_open_v1"
    checkpoint_path: "models/esm3/esm3_sm_open_v1_full.pth"
    strip_cls_eos: true
    embed_batch_size: 8
    combine_pairs: true
  lora:
    last_n_layers: 8
    target_modules: ["layernorm_qkv", "out_proj", "ffn"]
    r: 8
    alpha: 16
    dropout: 0.05
  mlp_head:
    dropout: 0.20
  regularization:
    dropout: 0.10
    cross_attention_dropout: 0.10
    projection_dropout: 0.10

training_config:
  epochs: 20
  batch_size: 8
  early_stopping_patience: 5
  monitor_metric: "auprc"
  logging:
    validation_metrics: ["auprc", "auroc", "f1", "accuracy"]
  optimizer:
    type: "adamw"
    lr: 0.00006
    beta1: 0.9
    beta2: 0.999
    eps: 1.0e-8
    weight_decay: 0.015
  scheduler:
    type: "onecycle"
    max_lr: 0.00006
    pct_start: 0.20
    div_factor: 25
    final_div_factor: 10000
    anneal_strategy: "cos"
  loss:
    type: "bce_with_logits"
    pos_weight: 1.0
    label_smoothing: 0.02
  strategy:
    type: "none"
  finetune:
    epochs: 30
    monitor_metric: "auprc"
    optimizer:
      lr: 0.000006
    loss:
      pos_weight: 7.0
      label_smoothing: 0.005
    strategy:
      type: "staged_unfreeze"
      unfreeze_epoch: 3
      initial_trainable_prefixes: ["output_head"]

evaluate:
  metrics:
    - "auroc"
    - "auprc"
    - "accuracy"
    - "sensitivity"
    - "specificity"
    - "precision"
    - "recall"
    - "f1"
    - "mcc"
